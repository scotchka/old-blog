<!DOCTYPE html>
<html lang="en">
<head>
        <title>Henry's Data Miscellany : How predictable is the New York Times?</title>
        <meta charset="utf-8" />
        <link rel="stylesheet" href="http://scotchka.github.io/theme/css/main.css" type="text/css" />
        <link href="http://scotchka.github.io/" type="application/atom+xml" rel="alternate" title="Henry's Data Miscellany ATOM Feed" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="http://scotchka.github.io/css/ie.css"/>
                <script src="http://scotchka.github.io/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="http://scotchka.github.io/css/ie6.css"/><![endif]-->

        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            imageFont: null,
            messageStyle: "none", 
            extensions: ["tex2jax.js"],
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                  inlineMath: [ ['$','$'] ],
                  displayMath: [ ['$$','$$'] ],
            processEscapes:true
            }
          });
        </script>



</head>

<body>
        
<header>
    <h1><a href="http://scotchka.github.io" id="site-title">Henry's Data Miscellany </a> :
        <a href="http://scotchka.github.io/how-predictable-is-the-new-york-times.html" id="page-title">How predictable is the New York Times?</a></h1>
<time datetime="2013-10-15T00:00:00">Tue 15 October 2013</time></header>
<article>
    <h1>Guess the Section</h1>
<p>Copy and paste the text from a New York Times article below and click "Guess" to see the predicted section. Try text from other newspapers, or any text for that matter, to see what section of the Times it would appear in.</p>
<form method="post" action="http://ec2-54-219-128-173.us-west-1.compute.amazonaws.com:5000/predict">
<textarea name='article_text' rows="25" cols="100"></textarea><br>
<input type="submit" value="Guess">   
</form>

<h1>Problem</h1>
<p>If you are given the text of a New York Times article, you can likely guess what section that article appears in. You'll immediately recognize the article as, say, a movie review or a news piece about a scientific discovery. The typical Times reader can discern the subject matter and match that to the appropriate section. Our task is to train a computer to do this.</p>
<h1>Naive Bayes</h1>
<p>In general, Bayesian estimation assigns probabilty to a scenario based on a set of observations. For example, if you observe the words "flavor", "cuisine", "restaurant" appearing frequently in an article, you may reasonably guess that it belongs to the "Dining &amp; Wine" section. Of course, a human does not actually look at the individual words, but rather larger structures such as phrases, sentences, or paragraphs. But for the sake of implementing an algorithm, we take the observation to be the individual words in an article and how often each appears. This simplication makes our method "naive" because each word is independent of the other words. In reality, "debt" and "ceiling" appearing together suggests that the article is about US politics, but the assumption of independence does not take this into account.</p>
<h1>Bag of Words</h1>
<p>Our data consist of 1000 recent Times articles from 27 sections. For each article, we tokenize the text, that is, break it up into a list of words. There are many decisions to make, such as whether to keep certain punctuations, foreign characters, etc. For the sake of simplicity, we strip away anything that is not a letter in the English alphabet, split by white space, and make all letters lower case. We also take the additional step of removing "stop words", which are common words such as "the" that do not distinguish one article from another. The end result is a so-called "bag of words", a table of counts for each word within each article.</p>
<h1>Conditional Probabilities</h1>
<p>Following Bayes Theorem, the probability of a section S given an article A is proportional to the probability of A given S times the prior probabilty of S:</p>
<p>$$ P(S|A) \propto P(A|S) \times P(S) $$</p>
<p>The prior $P(S)$ is readily obtained since it is just the fraction of the 1000 articles that are in section S. The conditional probability $P(A|S)$ is a rather bizarre object. There are an infinite number of possible articles in a section, so the probabilty of any particular article is zero! Here is where the naive assumption saves us: the independence of the words factorizes $P(A|S)$ into a product of probabilites for the words:</p>
<p>$$P(A|S) = P(\textrm{''tree''}|S) \times P(\textrm{''cat''}|S) \times ...$$</p>
<p>And so on for each word in the article. The conditional probability for a single word given a section S is easy to compute: just sum the counts of that word in the bag of words across all articles in the section, and then divide by the sum of counts of all words in the section. Do this for every word in the article, and multiply all the conditional probabilities and the prior together. This is the posterior $P(S|A)$, up to a constant coefficient.</p>
<p>Finally, do this for all sections, and choose the section that yields the highest posterior. This is the prediction.</p>
</article>

        <footer>
            <nav>
                <ul>
                </ul>
            </nav>
                <p id="theme-credit"><a href="http://mathieu.agopian.info/mnmlist/theme.html">Th√®me mnmlist</a></p>
        </footer>

</body>
</html>